{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "\n",
    "class TableEntry:\n",
    "    def __init__(self, key, value):\n",
    "        self.key = key\n",
    "        self.value = value\n",
    "        self.in_table = True\n",
    "\n",
    "    def get_key(self):\n",
    "        return self.key\n",
    "\n",
    "    def get_value(self):\n",
    "        return self.value\n",
    "\n",
    "    def set_value(self, value):\n",
    "        self.value = value\n",
    "\n",
    "    def is_in_table(self):\n",
    "        return self.in_table\n",
    "\n",
    "    def set_to_removed(self):\n",
    "        self.key = None\n",
    "        self.value = None\n",
    "        self.in_table = False\n",
    "\n",
    "    def is_removed(self):\n",
    "        return (self.key is None) and (self.value is None) and not self.in_table\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hash Table With Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HashTableWithCount():\n",
    "                                        #####\n",
    "    def __init__(self, initial_capacity=(100)):     # We're gonna need min 100\n",
    "                                        #####\n",
    "        self.table_size = self.get_next_prime(initial_capacity) # make a table sized w a prime over 100\n",
    "        self.table = [None] * self.table_size                   # fill it with None\n",
    "        self.item_count = 0                 # a variable to track how many elements are added/removed\n",
    "        self.total_probes = 0               # a variable to track probes for the experiment\n",
    "\n",
    "    def get_next_prime(self, n):\n",
    "        # uhh, let's use our existing prime methods\n",
    "        candidate = (n+1)                       # starting with the next possible number\n",
    "        while not self.is_prime(candidate):     # check if it's prime\n",
    "            candidate += 1                      # if not, iterate\n",
    "        return candidate                        # return the next prime found\n",
    "\n",
    "    def isOdd(self, n):                         # a helper function to check if an integer is odd\n",
    "        if((n%2)==0):\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "        \n",
    "    # thank heck we made one of these earlier\n",
    "    def is_prime(self, n):\n",
    "        if(n==1 or n==0):                               # handles the cases of 1 and 0\n",
    "            return False\n",
    "        elif(n==2):                                     # handles the case of 2\n",
    "            return True\n",
    "        elif(not self.isOdd(n)):                        # handles all even numbers except 0 and 2\n",
    "            return False\n",
    "        for x in range(2,round(n**(1/2))+1):            # for every number x between 2 and sqroot n\n",
    "            if(self.isOdd(x)):                          # only checks odds cause we already checked 2\n",
    "                if((n%x)==0):                           # if n divides by x evenly\n",
    "                    return False                        # return false\n",
    "        return True                                     # if none divide evenly, it's a prime\n",
    "    \n",
    "    # def check_size(self, size): \n",
    "    # above is the original header; didn't end up using size though\n",
    "\n",
    "    def check_size(self):                               # i'm not sure why size is passed to this\n",
    "        size = self.table_size                          # size is table size\n",
    "        ratio = (self.item_count/size)                  # we get a ratio of how full the array is\n",
    "        return ratio                                    # return the ratio of count/size\n",
    "\n",
    "    def get_hash_index(self, key):\n",
    "        return (hash(key) % self.table_size)            # computes the first index of the given key\n",
    "\n",
    "    def is_hash_table_too_full(self):\n",
    "                              #####\n",
    "        if(self.check_size() > 0.95):                    # checks if the hash table is 70% full\n",
    "                              #####\n",
    "            return True                                 # returns True, it is to full, if over 0.7\n",
    "        else: return False                              # returns false if check returns 0.7 or less\n",
    "\n",
    "    def enlarge_hash_table(self):\n",
    "        # makes the table size the next prime number at least double the current size\n",
    "                                                                  #####\n",
    "        new_table_size = self.get_next_prime(round(self.table_size*1.05))\n",
    "                                                                  #####\n",
    "        old_table = self.table                              # grab the old table\n",
    "        self.table = [None] * new_table_size                # make a new empty table\n",
    "        self.table_size = new_table_size                    # set table_size variable\n",
    "        self.item_count = 0                                 # reset itemcount before adding\n",
    "        for entry in old_table:                             # for every entry in the old table\n",
    "            if entry is not None and entry.is_in_table():   # if it's not None and not removed\n",
    "                self.add(entry.get_key(), entry.get_value())# add it to the table\n",
    "\n",
    "    def reset_probe_count(self):\n",
    "        self.total_probes = 0                               # just sets probes to 0\n",
    "\n",
    "    def get_probes(self):\n",
    "        return self.total_probes                            # returns the probe total\n",
    "    \n",
    "    def get_item_count(self):\n",
    "        return self.item_count                              # returns the item count\n",
    "\n",
    "table = HashTableWithCount()\n",
    "\n",
    "# testing enlarge function\n",
    "#print(table.table_size)\n",
    "#table.enlarge_hash_table()\n",
    "#print(table.table_size)\n",
    "#table.enlarge_hash_table()\n",
    "#print(table.table_size)\n",
    "\n",
    "#print(table.is_prime(20))\n",
    "#print(table.isOdd(5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Probing With Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearProbingWithCount(HashTableWithCount):\n",
    "\n",
    "    # def probe(self, index, key)\n",
    "    # above is the old header but I didn't need \"key\" in my implementation\n",
    "    \n",
    "    def probe(self, index):\n",
    "        self.total_probes += 1                      # increment probe count\n",
    "        return ((index + 1) % self.table_size)      # return the next index\n",
    "    \n",
    "\n",
    "    # def locate(self, index, key):\n",
    "    # above is the old input; but I didn't need to pass index\n",
    "    def locate(self, key):\n",
    "        #self.total_probes += 1\n",
    "        index = self.get_hash_index(key)            # grab the first possible index\n",
    "        while self.table[index] is not None:        # stop if we hit an empty cell\n",
    "            if self.table[index].get_key() == key:  # if the key is at this index\n",
    "                return index                        # return the index it was found in\n",
    "            index = self.probe(index)               # use probe to search further indices\n",
    "        return None                                 # return none if we don't find the key\n",
    "    \n",
    "\n",
    "    def add(self,key,value):\n",
    "        index = self.get_hash_index(key)            # grab the index\n",
    "        while self.table[index] is not None:        # until we hit an empty\n",
    "            if self.table[index].get_key() == key:  # if it's already in there\n",
    "                self.table[index].set_value(value)  # update the value\n",
    "                return                              # don't run forever\n",
    "            index = self.probe(index)               # otherwise, probe til we find it\n",
    "        self.table[index] = TableEntry(key,value)   # insert the new entry\n",
    "        self.item_count += 1                        # increment num_items\n",
    "        self.reset_probe_count()                    # make sure adding doesn't alter-\n",
    "                                                    # test results by resetting\n",
    "        if self.is_hash_table_too_full():           # check if the table is reaching capacity\n",
    "            self.enlarge_hash_table()               # enlarge if needed\n",
    "\n",
    "\n",
    "    def remove(self, key):\n",
    "        index = self.locate(key)                    # pull the index of the given key using locate()\n",
    "        if index is not None:                       # if the item is found\n",
    "            self.table[index] = None                # remove the entry\n",
    "            self.item_count -= 1                    # decrement item count\n",
    "        else:                                       # if something goes wrong\n",
    "            print(\"Linear remove operation failed: Key not found\") # lmk\n",
    "\n",
    "\n",
    "    def get_value(self, key):\n",
    "        index = self.locate(key)                    # use locate to find the object\n",
    "        if index is not None:                       # if entry is found\n",
    "            entry = self.table[index]               # grab the found object\n",
    "            return entry.get_value()                # use TableEntry's get_value method\n",
    "        return None                                 # If the entry isn't found, return None\n",
    "    \n",
    "\n",
    "    def contains(self, key):\n",
    "        index = self.locate(key)                    # locate the index\n",
    "        if index is not None:                       # if we find the key\n",
    "            return True                             # return True\n",
    "        return False                                # else return False\n",
    "    \n",
    "\n",
    "    def is_empty(self):\n",
    "        return (self.item_count==0)                 # True if 0 items, False otherwise\n",
    "    \n",
    "\n",
    "    def get_size(self):\n",
    "        return self.table_size                      # returns the item count\n",
    "    \n",
    "    \n",
    "    def clear(self):\n",
    "        self.table = [None] * 101                   # empty the table, set it to default size\n",
    "        self.table_size = 101\n",
    "        self.item_count = 0                         # reset item count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double Hashing With Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleHashingWithCount(HashTableWithCount):\n",
    "\n",
    "    \n",
    "    def primary_hash(self, key):\n",
    "        \"\"\"First hash function.\"\"\"\n",
    "        return hash(key) % self.table_size\n",
    "    \n",
    "\n",
    "    def secondary_hash(self, key):\n",
    "        \"\"\"Second hash function to calculate step size.\"\"\"\n",
    "        return 1 + (hash(key) % (self.table_size - 1))\n",
    "    \n",
    "\n",
    "    def probe(self, index, key):\n",
    "        # I've set probe up to do the stepping, while Locate and Add will interpret\n",
    "        # the result returned by probe\n",
    "        step = self.secondary_hash(key)             # finds the step length\n",
    "        while (self.table[index] is not None) and (self.table[index].get_key() != key):\n",
    "                                                    # until we find the key, or an empty bucket\n",
    "            self.total_probes += 1                  # iterate probe count\n",
    "            index = (index + step)%self.table_size  # we keep stepping\n",
    "        return index                # we return the index of the given key or empty bucket\n",
    "    \n",
    "\n",
    "    # def locate(self, index, key): # don't think i'll need to pass index\n",
    "    def locate(self, key):\n",
    "        #self.total_probes += 1\n",
    "        index = self.primary_hash(key)                  # grabs the first index\n",
    "        while self.table[index] is not None:            # until we hit an empty\n",
    "                if self.table[index].get_key() == key:  # if the key is in the bucket\n",
    "                    return index                        # located, return the index\n",
    "                index = self.probe(index,key)           # probe and grab the index\n",
    "        return None                                     # if we hit an empty, return None\n",
    "    \n",
    "\n",
    "    def add(self,key,value):                            # same add method as Linear\n",
    "        index = self.primary_hash(key)                  # grab the first index\n",
    "        while self.table[index] is not None:            # until we hit an empty\n",
    "            if self.table[index].get_key() == key:      # if it's already in there\n",
    "                self.table[index].set_value(value)      # update the value\n",
    "                return                                  # don't run forever\n",
    "            index = self.probe(index,key)               # otherwise, probe til we find it\n",
    "        self.table[index] = TableEntry(key,value)       # insert the new entry\n",
    "        self.item_count += 1                            # increment num_items\n",
    "        self.reset_probe_count()                        # make sure adding doesn't alter-\n",
    "                                                        # test results by resetting\n",
    "        if self.is_hash_table_too_full():               # check if the table is reaching capacity\n",
    "            self.enlarge_hash_table()                   # enlarge if needed\n",
    "\n",
    "\n",
    "    def remove(self, key):                          # same remove method should do the trick\n",
    "        index = self.locate(key)                    # pull the index of the given key using locate()\n",
    "        if index is not None:                       # if the item is found\n",
    "            self.table[index] = None                # remove the entry\n",
    "            self.item_count -= 1                    # decrement item count\n",
    "        else:                                       # if something goes wrong\n",
    "            print(\"Double hash remove operation failed: Key not found\") # lmk\n",
    "\n",
    "\n",
    "    def get_value(self, key):                       \n",
    "        index = self.locate(key)                    # grab the appropriate index using locate\n",
    "        if index is not None:                       # if we find the item\n",
    "            return self.table[index].get_value()    # return its value\n",
    "        return None                                 # return none if not found\n",
    "    \n",
    "\n",
    "    def contains(self, key):                        # same method as Linear should work\n",
    "        index = self.locate(key)                    # locate the index\n",
    "        if index is not None:                       # if we find the key\n",
    "            return True                             # return True\n",
    "        return False                                # else return False\n",
    "    \n",
    "\n",
    "    def is_empty(self):                             # checks if the table is empty\n",
    "        return (self.item_count==0)                 # returns True if it's empty and False otherwise\n",
    "    \n",
    "\n",
    "    def get_size(self):\n",
    "        return self.table_size                      # returns the item count\n",
    "    \n",
    "\n",
    "    def clear(self):                                # flips the table over and gets a new one\n",
    "        self.table = [None] * 101                   # empty the table, set it to default size\n",
    "        self.table_size = 101\n",
    "        self.item_count = 0                         # reset item count\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name Generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# behold, a name generator way more complicated than it needed to be\n",
    "\n",
    "def name_generator(n1,n2):   \n",
    "\n",
    "    genList = []\n",
    "\n",
    "    v = 1\n",
    "    while (26**v)<(n1+n2):                  # find how many letters we need to satisfy both lists\n",
    "        v += 1                              # increment v (the number of letters we'll use)\n",
    "        \n",
    "    while (len(genList) < (n1+n2)):         # till genList is as many elements as called for\n",
    "        for j in range(2,v+1):              # generate all the names with j letters\n",
    "            genList.extend(recursy(j))      # til we reach v letters\n",
    "    \n",
    "    listN1 = genList[:n1]                   # make the first n1 elements listN1\n",
    "    listN2 = genList[n1:]                   # make the rest listN2\n",
    "    listN2 = listN2[:n2]                    # chop the list to n2 length\n",
    "\n",
    "    # Some diagnostic stuff:\n",
    "    #print(\"listN1 length: \",len(listN1)) # for diag\n",
    "    #print(\"listN2 length: \",len(listN2)) # for diag\n",
    "    #print(listN1)\n",
    "    #print(listN2)\n",
    "\n",
    "    return (listN1,listN2)\n",
    "\n",
    "def recursy(lCount = int, returnList = None, rCount = 0, name_string = \"\"):\n",
    "    charlist = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r',\n",
    "                's','t','u','v','w','x','y','z']\n",
    "    # returns a list of all letter combinations with lCount number of characters\n",
    "                                            # recurses to lCount depth\n",
    "                                            # each appends their letter to name_string\n",
    "    if returnList is None:                  # if there isn't yet a returnlist\n",
    "        name_string = \"\"                    # set name string\n",
    "        returnList = []                     # set return list\n",
    "\n",
    "    for c in charlist:                      # for every char in the alphabet                 \n",
    "        new_string = name_string + c        # append the char to the passed name_string\n",
    "        rCount = rCount+1                   # iterate rCount\n",
    "        if (rCount == lCount):              # if it's the deepest instance of recursion\n",
    "            returnList.append(new_string)   # add completed name to return list\n",
    "        else:                               # if it's not the deepest instance\n",
    "                                            # call recursy again\n",
    "            recursy(lCount, returnList, rCount, new_string)\n",
    "        rCount -= 1                         # decrement rCount after recursion\n",
    "                                            # once all the looping mumbo is done\n",
    "    return returnList                       # return the list\n",
    "\n",
    "    \n",
    "# print(recursy(lCount=2))                  # testing\n",
    "# print(name_generator(100,1000))           # testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear hash probe counts:  [31566, 35781, 29045, 24954, 23538, 25002, 32463, 17918, 22450, 31992, 23756, 26395, 26827, 27027, 14331, 34993, 35018, 29021, 20255, 36115, 13309, 24780, 38419, 36034, 30535, 25165, 15381, 19044, 20985, 26557, 27549, 34096, 33402, 29919, 33368, 40035, 43223, 34824, 12633, 21840, 26837, 24756, 27269, 42491, 23775, 24764, 40127, 22578, 39748, 19973, 14221, 34948, 35993, 22098, 33274, 25484, 17995, 33345, 21679, 15673, 22048, 40351, 18569, 26935, 16393, 20613, 15232, 39008, 14381, 19200, 20523, 33653, 33210, 42437, 22725, 33831, 39550, 22118, 37204, 30612, 19924, 26420, 20242, 35324, 19350, 16687, 37463, 21347, 15702, 20780, 20107, 12751, 24106, 43175, 45946, 14656, 28585, 25496, 19031, 15971, 20346, 42023, 26292, 44437, 38479, 24880, 15903, 20390, 22153, 39893, 39788, 22211, 25166, 45732, 38671, 18147, 30811, 44890, 29893, 23456, 38661, 15384, 21997, 21689, 37306, 22405, 23945, 26694, 12493, 39899, 43176, 29766, 45094, 22389, 25779, 41450, 25843, 28815, 38037, 16618, 16093, 21454, 45446, 33324, 30920, 31586, 22757, 13961, 41186, 17403, 30613, 15829, 20644, 14920, 17484, 22911, 22065, 42287, 40721, 19673, 27245, 16395, 21397, 36297, 33710, 29150, 19621, 15248, 26951, 17242, 10377, 41415, 19869, 38001, 22234, 22094, 31938, 45790, 25558, 15313, 18846, 13985, 26345, 31652, 40455, 25379, 22766, 21408, 35022, 29977, 15373, 20969, 26710, 34467, 24048, 32252, 21126, 16900, 31389, 23343, 25616, 35268, 19816, 39226, 31299, 35050, 21480, 42645, 32198, 31541, 46006, 20532, 22940, 41944, 42684, 20120, 28171, 33667, 34441, 13327, 17316, 30766, 11454, 38162, 22567, 33454, 35447, 33018, 13387, 28501, 19841, 25687, 42682, 19914, 17692, 32131, 15885, 22724, 39840, 31776, 18378, 45895, 37708, 41529, 41317, 26823, 33081, 44300, 26541, 38309, 20793, 17235, 18187, 26940, 46027, 38285, 29652, 26922, 31849, 42101, 42165, 20640, 36635, 15885, 40001, 17766, 15274, 12037, 20197, 29717, 13326, 31318, 15403, 23214, 25351, 25278, 23021, 44974, 20835, 44377, 16704, 32642, 14558, 39418, 16253, 45879, 36182, 43506, 17048, 40880, 21982, 23842, 22479, 30673, 18625, 15640, 20419, 19949, 23905, 19004, 20813, 16138, 36269, 35772, 17316, 31800, 33063, 21324, 21145, 40624, 15910, 16207, 28222, 18819, 23664, 44332, 11032, 30298, 21525, 23493, 23042, 27492, 45292, 16962, 23096, 42062, 32360, 22411, 38835, 28583, 18322, 29362, 25690, 26944, 26435, 17221, 31279, 40929, 26179, 18308, 22719, 35120, 18330, 37881, 40276, 38577, 22251, 26242, 24824, 20972, 17094, 39477, 26775, 20970, 41051, 39905, 22401, 31725, 30704, 36291, 29104, 46130, 16907, 26677, 15166, 23588, 29764, 23953, 14944, 33634, 18826, 36754, 16292, 20160, 38165, 12698, 21030, 22183, 17125, 42121, 25994, 37450, 43108, 13682, 20544, 19650, 36961, 19896, 47916, 41617, 22325, 25528, 34288, 15140, 23917, 15286, 31017, 23438, 12149, 23988, 35164, 28751, 15454, 31171, 35096, 19114, 35861, 12227, 29855, 19819, 35923, 20384, 43435, 15825, 27214, 21301, 28850, 21913, 28190, 17530, 37435, 31048, 12854, 40893, 35875, 17556, 27470, 22687, 23651, 43637, 29077, 17285, 13644, 26175, 22873, 24500, 45514, 28675, 14391, 23696, 39991, 23192, 21106, 27821, 22504, 30900, 16352, 22335, 41860, 36531, 18935, 27507, 31812, 24114, 35480, 23862, 33434, 31164, 42158, 36143, 35596, 38338, 24042, 24320, 34460, 15837, 37041, 16775, 27789, 38027, 18627, 28903, 15672, 26179, 20902, 15385, 38871, 38054, 17571, 38945, 17263, 18831, 28734, 22230, 22891, 33826, 26559, 10577, 12301, 33721, 39646, 46242, 19340, 37835, 20132, 29836, 20123, 19072, 19633, 39093, 47493, 20531, 23026, 20110, 17269, 30625, 21540, 29537, 27897, 25008, 35716, 16584, 32725, 22460, 28710, 25450, 27133, 40287, 30941, 28480, 32292, 25206, 37008, 22430, 27394, 37980, 30429, 40510, 26163, 37569, 35107, 44168, 34691, 26968, 19483, 42987, 35914, 19500, 15985, 27530, 26296, 36755, 21546, 25108, 34906, 25311, 33640, 35370, 34360, 22556, 46381, 19816, 36153, 27224, 16616, 22184, 13936, 25757, 25507, 24604, 30331, 37202, 36637, 29926, 23285, 39558, 26905, 41413, 23311, 14907, 13287, 15856, 34367, 21913, 30185, 20240, 37580, 27574, 23344, 42785, 22513, 36803, 32659, 20680, 36499, 40921, 16906, 21973, 24204, 25422, 30037, 32209, 18151, 19518, 31774, 21817, 24669, 28856, 29104, 16019, 23503, 25759, 33826, 19173, 43396, 21408, 34876, 19779, 30924, 34964, 26149, 17317, 42105, 31027, 13099, 34573, 39618, 19439, 26777, 36720, 23346, 25919, 26168, 38836, 41599, 19718, 28588, 41045, 35088, 41820, 21680, 23550, 16260, 32638, 29193, 33228, 24002, 35410, 38580, 34775, 21487, 41408, 31359, 21277, 17748, 17711, 45288, 36622, 44731, 33199, 34332, 13533, 37465, 21817, 10520, 11072, 22353, 26386, 36979, 47122, 43917, 36988, 36370, 33139, 26336, 21670, 20007, 32774, 26074, 43259, 15165, 13786, 29224, 28479, 21601, 33162, 43085, 34271, 14462, 18436, 26217, 33238, 30883, 18888, 29733, 28826, 38395, 25867, 23156, 32645, 17776, 15380, 24759, 28656, 34625, 20310, 26784, 17683, 48470, 19343, 27878, 14066, 34623, 26989, 11037, 19901, 21024, 34779, 43920, 35230, 20086, 17015, 44069, 24680, 18891, 30014, 15409, 17072, 27404, 41398, 43688, 34946, 28440, 25697, 26623, 23574, 32796, 13386, 25898, 18466, 38987, 38606, 32763, 24657, 28908, 37426, 23646, 38208, 21603, 23434, 38831, 23195, 23272, 25581, 17204, 40234, 28985, 35897, 22460, 27219, 37650, 33115, 13286, 34888, 15686, 24260, 46296, 20959, 12651, 23579, 35517, 26819, 15894, 44536, 19492, 42686, 15933, 14435, 36041, 46415, 17600, 25572, 40400, 44468, 16286, 21353, 18759, 25249, 34487, 46515, 22395, 15418, 22837, 14691, 25645, 28712, 24552, 27829, 27914, 18712, 24365, 23736, 36356, 29022, 26422, 36545, 37922, 35405, 22043, 32675, 33880, 37564, 13191, 26566, 15001, 40455, 33097, 39015, 23897, 33009, 29592, 29142, 35992, 18727, 39991, 28158, 23188, 29785, 21135, 43110, 33587, 25374, 19749, 21693, 16747, 16654, 28662, 28900, 37175, 22095, 29114, 33523, 39245, 19027, 27673, 12765, 24328, 20743, 28373, 26926, 20316, 31503, 14124, 16944, 34453, 36828, 22351, 27800, 30022, 40686, 41866, 38559, 18022, 22794, 17730, 23506, 46299, 12977, 12825, 23021, 17741, 23181, 26371, 23761, 20567, 35815, 35060, 31148, 16306, 45853, 23893, 24075, 17170, 32713, 43456, 38159, 16566, 34456, 25313, 12531, 22520, 22097, 16531, 21600, 19631, 10288, 26377, 22636, 37213, 34013, 38303, 20216, 25073, 18099, 36488, 38603, 21152, 24716, 28768, 33446, 38452, 23314, 25129, 43305, 20089, 15127, 36412, 38326, 19848, 26981, 25294, 43410, 23697, 22271, 30601, 37171, 37938, 36423, 42126, 26433, 31028, 37640, 34615, 19504, 47821, 43929, 43044, 22804, 24380, 40394, 15281, 35939, 24766, 17089, 28962, 20293, 32236, 19323, 14771, 20970, 26077, 26234, 13921, 27873, 17091, 26440, 36891, 17620, 19558, 30277, 29976, 16859, 15470, 38515, 16234, 24751, 36701, 29784, 19274, 27512, 29200, 30141, 21737, 24886, 42618, 42885, 16796, 32084, 21935, 24690, 48455, 22961, 42323, 36148, 29773, 17706, 42459, 28606, 44872, 15557, 22572, 25794, 32353, 32680, 34739, 23294, 19778, 33456, 30370, 15997, 17231, 14779, 32900, 29404, 14993, 27890, 17066, 20893, 33616, 19529]\n",
      "double hash probe counts:  [12967, 12327, 12556, 12657, 12513, 12799, 12362, 11751, 12493, 12982, 13023, 12730, 12374, 13159, 12291, 12691, 12431, 12759, 12187, 12547, 12644, 12398, 12604, 12287, 12510, 13141, 11965, 12626, 12245, 12930, 12735, 12710, 12413, 12399, 12557, 11801, 11991, 11844, 12156, 12134, 12002, 11740, 12733, 12590, 12641, 11936, 12717, 12593, 12676, 12100, 12604, 12208, 12630, 12538, 12032, 12449, 12340, 12569, 13231, 12566, 11797, 12432, 11736, 12498, 12795, 12896, 12334, 11926, 12439, 12409, 12041, 12369, 13062, 13095, 12772, 12134, 13116, 12386, 12308, 12569, 12350, 12337, 12170, 13341, 12672, 13379, 12641, 12234, 12666, 12497, 12403, 12374, 12572, 13190, 12086, 12101, 12351, 12512, 12300, 13010, 12715, 13068, 12035, 13099, 12324, 12718, 12797, 12678, 12308, 12695, 12532, 12416, 12870, 13093, 12713, 13161, 12865, 12893, 12717, 12946, 12714, 12469, 11957, 13092, 12607, 12719, 12753, 12062, 12536, 12101, 12241, 12091, 11817, 12655, 12897, 12525, 11812, 12851, 12821, 12114, 12863, 12050, 12958, 12448, 12258, 12419, 12701, 12512, 12173, 12436, 11793, 13199, 12430, 12776, 12408, 12332, 12838, 11801, 12194, 12220, 12747, 12839, 12603, 12845, 12690, 13430, 11916, 13056, 12790, 12307, 12087, 12375, 12179, 12908, 12976, 12282, 12545, 12217, 12063, 12149, 12447, 11548, 12701, 13110, 12504, 12364, 12456, 12184, 12658, 12368, 12623, 11279, 12381, 12837, 12358, 12565, 12846, 12654, 13061, 12706, 12646, 12169, 12652, 12444, 12671, 13566, 12273, 12220, 12410, 12200, 12556, 11753, 12225, 13456, 12580, 12525, 12843, 12733, 12491, 12762, 12400, 12326, 12066, 12995, 12175, 12238, 12237, 13101, 12211, 12024, 12898, 13295, 12182, 12450, 12942, 12443, 13031, 12672, 12481, 13052, 12777, 12962, 12417, 12382, 12705, 12005, 11883, 12747, 13393, 12416, 12443, 13610, 12546, 13933, 12785, 12711, 12177, 12762, 12558, 12013, 12548, 12096, 12069, 11727, 12065, 12094, 12249, 12377, 12519, 12628, 12877, 12451, 12280, 12617, 12306, 12568, 12179, 12463, 12891, 12221, 12216, 12399, 12328, 12496, 12258, 12431, 12110, 12194, 12671, 12222, 12740, 11957, 12399, 11935, 11976, 12614, 11964, 12333, 12506, 12340, 11749, 12831, 11573, 13881, 12575, 13187, 12327, 11640, 12461, 11848, 12925, 12013, 12288, 12484, 12643, 11610, 11843, 12647, 12737, 12017, 11716, 12760, 12897, 12744, 12534, 12701, 12500, 12394, 13158, 12782, 12710, 12591, 12614, 13156, 12004, 12318, 11437, 12768, 12495, 12455, 12371, 12707, 12492, 12934, 12436, 12588, 11889, 12215, 11733, 11851, 13261, 11849, 12089, 12079, 12344, 12360, 11810, 12758, 12512, 12545, 12749, 13061, 12987, 11738, 12408, 12069, 12847, 13693, 12177, 12233, 12800, 12946, 12694, 12894, 12472, 12154, 12799, 13141, 11611, 12544, 12188, 12653, 12414, 12737, 12335, 13152, 12951, 12615, 12512, 12586, 13194, 12285, 13060, 13006, 12339, 12619, 12554, 12139, 12434, 12604, 12670, 12368, 12629, 12303, 11673, 11578, 12641, 12600, 12007, 12161, 12222, 13067, 12437, 13021, 12666, 12774, 11827, 13157, 12321, 12839, 12957, 12927, 12678, 11406, 12318, 12819, 12003, 12828, 12066, 12475, 13104, 12553, 12173, 12391, 12058, 12345, 11856, 12541, 12583, 12195, 12341, 12702, 12389, 12568, 12582, 12424, 11460, 13353, 12732, 12664, 12658, 12921, 13152, 12931, 12250, 12435, 12098, 12415, 12266, 12850, 12404, 12671, 12749, 12336, 12916, 12539, 12768, 12796, 12635, 13028, 12541, 12757, 12174, 12673, 13185, 13045, 12472, 12302, 12183, 12484, 11953, 12667, 12277, 12415, 12410, 12408, 12951, 13221, 12353, 12377, 12548, 12889, 12617, 12618, 12446, 12340, 11885, 12005, 12574, 12929, 12385, 12913, 12639, 12155, 12829, 13456, 11915, 12322, 12518, 13270, 12096, 12101, 12323, 12213, 12215, 12422, 13364, 12927, 12677, 12207, 12323, 12474, 11957, 12331, 12882, 12849, 13069, 13244, 12971, 12905, 12546, 13301, 12966, 11620, 12744, 12124, 12911, 11764, 12567, 12993, 12431, 13211, 11857, 12781, 12714, 12323, 13551, 12692, 13107, 12492, 12773, 12148, 12385, 12491, 12028, 12440, 12493, 12847, 12665, 13254, 12615, 12131, 12378, 12649, 12137, 11674, 12568, 13463, 12773, 13117, 13649, 12541, 11962, 13051, 12566, 12751, 12596, 12670, 11865, 12329, 12745, 12797, 13112, 12392, 12665, 12906, 12685, 12353, 12788, 12501, 12404, 12489, 13023, 12649, 12068, 12757, 12188, 12479, 13102, 12805, 12780, 13072, 12145, 12275, 12243, 12606, 12888, 12351, 12256, 12752, 12104, 12630, 12495, 12695, 12647, 13032, 12473, 13326, 12583, 12503, 12796, 13127, 13227, 12270, 11767, 13386, 13493, 12399, 12257, 13124, 11924, 12065, 12933, 12565, 11601, 12496, 12003, 12542, 12808, 12636, 12642, 13066, 12167, 12466, 12246, 12259, 12440, 12369, 12775, 11903, 11834, 12035, 12436, 12810, 12693, 12227, 12152, 12456, 12021, 12372, 12687, 12299, 12880, 12515, 13066, 12228, 12351, 12104, 12611, 12351, 12018, 12820, 12230, 12383, 12404, 12565, 12855, 14233, 12656, 12291, 12933, 12126, 12050, 13174, 12563, 12862, 11985, 12771, 13316, 12781, 13086, 12244, 12597, 12731, 12912, 13002, 12214, 12381, 12904, 12845, 12432, 11744, 12814, 12246, 12326, 12770, 12413, 12357, 12605, 12254, 12082, 12360, 11962, 12819, 12387, 11728, 12413, 12359, 12630, 12696, 13141, 12481, 12687, 11823, 11783, 13166, 13068, 12726, 12396, 12093, 11863, 12085, 13040, 11687, 12439, 13300, 13009, 13045, 12754, 12556, 12260, 13549, 12345, 12620, 12832, 12628, 12274, 12378, 12820, 12790, 12229, 13235, 13441, 12384, 12187, 12236, 12487, 12675, 12556, 12291, 12898, 12431, 12231, 12333, 12317, 12328, 12193, 13184, 12529, 12554, 12805, 12125, 12498, 13674, 13306, 12352, 12791, 12910, 13234, 13130, 12271, 12378, 12940, 12025, 12388, 12476, 12742, 12826, 12781, 12366, 12670, 12001, 11710, 13297, 12960, 12532, 12582, 12795, 12921, 12617, 12212, 12489, 12038, 12937, 12726, 12577, 12682, 12316, 12131, 12961, 12359, 12511, 12752, 12609, 11856, 12640, 13563, 13080, 12402, 12901, 13212, 11810, 12728, 11887, 12617, 12608, 12657, 12415, 12834, 13109, 12347, 12313, 12399, 12768, 12477, 12704, 12437, 13549, 12526, 12257, 13047, 12871, 12604, 11717, 12616, 12820, 12633, 12355, 12569, 12929, 12738, 12702, 12795, 12191, 12466, 11985, 12629, 12622, 11912, 13046, 12187, 12538, 12818, 12804, 12575, 12816, 12719, 12152, 12583, 12883, 12298, 12363, 12690, 12149, 12986, 12320, 12291, 12682, 12689, 12034, 12493, 12180, 12404, 12966, 12770, 12419, 11903, 12410, 12701, 13222, 12625, 12327, 12032, 12608, 12431, 11907, 12483, 11990, 11965, 11647, 12562, 12831, 12344, 11574, 12100, 13110, 13385, 12756, 12383, 12145, 13000, 12505, 12220, 12598, 12775, 12077, 13244, 12566, 12488, 12461, 12763, 13098, 12115, 12424, 12395, 12704, 12818, 12751, 12053, 13325, 13194, 12423, 12374, 11734, 13316, 12077, 12113, 12401, 12215, 12474, 12245, 13076, 12198, 12347, 12686, 12328, 12257, 12246, 12950, 11539, 12535, 13275, 12386, 11905, 12700, 12640, 12028, 12404, 13037, 12939, 11724, 12529, 12757, 13334, 12046, 12883, 11868, 13297, 12454, 12658, 12838, 13007, 11834, 11878, 12100, 12561, 12911, 12578, 12767, 13052, 12300, 12535, 14068, 12504, 12540, 12042, 12505, 13472, 13053, 12561, 12846, 12547, 13202, 12055, 13128, 12053, 12439, 12336, 12292, 12314]\n",
      "linear hash table size:          107\n",
      "double hash table size:          107\n",
      "linear hash probe count mean:    27660.758\n",
      "linear hash avg per search:      27.660758\n",
      "linear hash standard deviation:  9022.12239941501\n",
      "double hash probe count mean:    12531.228\n",
      "double hash avg per search:      12.531227999999999\n",
      "double hash standard deviation:  412.00776985199974\n"
     ]
    }
   ],
   "source": [
    "import random   \n",
    "import math\n",
    "# we're allowed to import random and math right?\n",
    "# both of these get imported in TableEntry and they're from the standard library, so I'm just going to assume\n",
    "\n",
    "class GetStatistics:\n",
    "\n",
    "    @staticmethod\n",
    "    def main():                                             # this class does science\n",
    "\n",
    "        # The results requested in the problem text in the form of an excerpt from my final notes at the end-\n",
    "        # of the big long markdown that contains all the experiment data I thought was pertinent\n",
    "        \"\"\"\n",
    "        The data I've collected suggests that the double hashing method is able to achieve nearly 1.5 PPS (Probes Per Search)\n",
    "        with a table size of only 163, while the linear hash method needs a table of size 197 to get to 1.5 PPS.\n",
    "        \"\"\"\n",
    "        # I talk in detail about why I think this might be in the notes at the bottom of the big markdown down there\n",
    "\n",
    "\n",
    "\n",
    "        linear_ht = LinearProbingWithCount()                # Our linear hash table\n",
    "        double_ht = DoubleHashingWithCount()                # our double hash table\n",
    "\n",
    "        linear_count = []                                   # list for linear prob stats\n",
    "        double_count = []                                   # list for double hash stats\n",
    "\n",
    "        names = GetStatistics.generate_names()              # a list to hold 11k three letter names\n",
    "        add_list = GetStatistics.get_1000_names(names)      # a list of names to add from\n",
    "        search_list = GetStatistics.get_10000_names(names)  # a list of names to search for\n",
    "\n",
    "\n",
    "        for t in range(1000):\n",
    "            GetStatistics.choose_100_names_and_add(linear_ht, double_ht, add_list)  # add names from add list\n",
    "            GetStatistics.search_for_100_names(linear_ht,double_ht,search_list)     # search names\n",
    "            linear_count.append(linear_ht.get_probes())                             # add probe count from-\n",
    "            double_count.append(double_ht.get_probes())                             # this test iteration\n",
    "            if t == 999:\n",
    "                print(\"linear hash probe counts: \", linear_count)           # print our probe count lists\n",
    "                print(\"double hash probe counts: \", double_count)\n",
    "                print(\"linear hash table size:         \", linear_ht.get_size())     # print table sizes\n",
    "                print(\"double hash table size:         \", double_ht.get_size())\n",
    "            linear_ht.clear()\n",
    "            double_ht.clear()\n",
    "\n",
    "\n",
    "        GetStatistics.compute_statistics(linear_count,double_count)  # call compute stats\n",
    "        \n",
    "\n",
    "    # i got tunnel vision and made a complicated goofy name generator\n",
    "    # as a result we need an extra method to bodge things together\n",
    "    @ staticmethod\n",
    "    def generate_names():   # the first 676 names will be 2 char, so let's skip em for consistency\n",
    "                            # then grab the 11k after those, so they're all 3 character strings\n",
    "        geny = name_generator(676,11676) \n",
    "        return geny[1]      # skip the 2 letter ones, return the list of 3 letter names\n",
    "    \n",
    "    @staticmethod\n",
    "    def choose_100_names_and_add(linear_ht, double_ht, add_list):\n",
    "        # randomly chooses 100 names from the 1000\n",
    "        # adds em to both linear_ht and double_ht\n",
    "        one_hundred_names = random.sample(add_list, 100)    # grab a random sample of names\n",
    "        for name in one_hundred_names:                      # for each name in our random sample\n",
    "            linear_ht.add(name,name)                        # add to linear_ht\n",
    "            double_ht.add(name,name)                        # add to double_ht\n",
    "        linear_ht.reset_probe_count()                       # reset the probe count since adding might use probes\n",
    "        double_ht.reset_probe_count()                       # do the same for double_ht\n",
    "\n",
    "    @staticmethod\n",
    "    def get_1000_names(names):\n",
    "        return names[:1000]                                 # grab the first 1000 names from list\n",
    "        \n",
    "    @staticmethod\n",
    "    def get_10000_names(names):\n",
    "        return names[1000:]                                 # grab the next 10k names from list\n",
    "\n",
    "    @staticmethod\n",
    "    def search_for_100_names(linear_ht,double_ht,search_list):  # changed names to search_list here\n",
    "        one_hundred_names = random.sample(search_list, 1000)    # grab a random sample of 100\n",
    "        for name in one_hundred_names:                          # for every name in the list\n",
    "            linear_ht.locate(name)                              # search linear\n",
    "            double_ht.locate(name)                              # search double\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_statistics(linear_count, dhash_count):          # this method is for \n",
    "        linear_mean = sum(linear_count) / len(linear_count)\n",
    "        double_mean = sum(dhash_count) / len(dhash_count)             # get our means and print them\n",
    "\n",
    "        linear_diffs = [(i - linear_mean) ** 2 for i in linear_count] # compute difs from mean\n",
    "        double_diffs = [(i - double_mean) ** 2 for i in dhash_count]    \n",
    "\n",
    "        l_variance = sum(linear_diffs) / (len(linear_count) - 1)      # compute variance\n",
    "        d_variance = sum(double_diffs) / (len(dhash_count) -1)\n",
    "\n",
    "        linear_standard_dev = math.sqrt(l_variance)                   # sqrt variance for std dev\n",
    "        double_standard_dev = math.sqrt(d_variance)\n",
    "\n",
    "        print(\"linear hash probe count mean:   \", linear_mean)        # print all our results\n",
    "        # print(\"linear hash table size: \", GetStatistics.linear_ht.get_size())\n",
    "        print(\"linear hash avg per search:     \", linear_mean/1000)\n",
    "        print(\"linear hash standard deviation: \", linear_standard_dev)\n",
    "        print(\"double hash probe count mean:   \", double_mean)\n",
    "        # print(\"double hash table size: \", GetStatistics.double_ht.get_size())\n",
    "        print(\"double hash avg per search:     \", double_mean/1000)\n",
    "        print(\"double hash standard deviation: \", double_standard_dev)\n",
    "        \n",
    "        pass\n",
    "        \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    GetStatistics.main()\n",
    "\n",
    "                            # Experiment notes below the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below is GetStatistics test/diag code that's been cut out\n",
    "\n",
    "# print(\"lcount length: \", len(linear_count))\n",
    "# print(\"dhcount length: \", len(dhash_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TROUBLESHOOTING:\n",
    "(Not all the testing was recorded here, but everything I thought important was)\n",
    "\n",
    "    initial test results (load ratio 0.7):\n",
    "        linear hash probe count mean:  50.408\n",
    "        linear hash standard deviation:  10.74928291017978\n",
    "        double hash probe count mean:  39.857\n",
    "        double hash standard deviation:  7.694137427337177\n",
    "        ####\n",
    "        linear hash probe count mean:  50.322\n",
    "        linear hash standard deviation:  10.033692789346851\n",
    "        double hash probe count mean:  40.823\n",
    "        double hash standard deviation:  7.510264647472489\n",
    "\n",
    "    Hmmmm, I guess I'll have to tune the is_table_too_full ratio value to try and get these numbers to be more similar because it looks like double hash probing is about 20% faster here\n",
    "\n",
    "    Test Batch 2 (load ratio 0.5):\n",
    "        linear hash probe count mean:  50.374\n",
    "        linear hash standard deviation:  10.614084170703697\n",
    "        double hash probe count mean:  39.923\n",
    "        double hash standard deviation:  7.5682461210870615\n",
    "        ####\n",
    "        linear hash probe count mean:  51.217\n",
    "        linear hash standard deviation:  10.873092252672885\n",
    "        double hash probe count mean:  40.245\n",
    "        double hash standard deviation:  7.764487250441092\n",
    "\n",
    "    Nope, double hash still seems to be 20% faster. I think that makes sense, cause the whole point of double hashing is avoiding collisions, but I guess the example results had me thinking they were going to be closer.\n",
    "\n",
    "    I've been messing around with load ratios and table sizes for a bit trying to get the results to shift, but they've remained very close to 50 and 40 average probes respectively. I'm not sure why I'm not seeing a shift. Might add a function that computes the average number of probes per individual search.\n",
    "\n",
    "    I think the way I currently have it set up is blowing up to the same table size everytime. I've got to change the enlarge method so that the load ratios will matter.\n",
    "\n",
    "    That didn't seem to do anything either. Still getting the same results... That doesn't make sense.\n",
    "\n",
    "    OKAY, finally figured out what the problem was; I need to run all the classes individually each time a change is made to any of them, I guess. Changing the table full ratio to 0.9 results in the following:\n",
    "\n",
    "    linear hash probe count mean:  54.115\n",
    "    linear hash standard deviation:  11.04190647934942\n",
    "    double hash probe count mean:  46.048\n",
    "    double hash standard deviation:  8.561015534794162\n",
    "    \n",
    "    If we want an average of at least 1.5 probes per failed search, we'd be looking at 150 probes per test, which isn't close to what I've been getting. I'm gonna start messing around with a few numbers: the initial capacity, the load threshhold for enlarging the tables, and the amount they enlarge by\n",
    "\n",
    "    (Initial Cap: 10, Enlarge when load > 0.7, Enlarge by > 2x)\n",
    "    linear hash probe count mean:  53.144\n",
    "    linear hash standard deviation:  10.702981725566843\n",
    "    double hash probe count mean:  45.566\n",
    "    double hash standard deviation:  8.50764503396939\n",
    "\n",
    "    (Initial Cap: 10, Enlarge when load > 0.7, Enlarge by > 1.2x)   This should have made\n",
    "    linear hash probe count mean:  12.53                            the number of probes larger\n",
    "    linear hash standard deviation:  3.7038923875861753             but it didn't which makes\n",
    "    double hash probe count mean:  12.221                           me think something is going\n",
    "    double hash standard deviation:  3.584272032924678              wrong with probe count resets\n",
    "\n",
    "    (Initial Cap: 10, Enlarge when load > 0.7, Enlarge by > 1.2x)   No, no, wait, that doesn't make\n",
    "    linear hash probe count mean:  16.564                           sense either because it only\n",
    "    linear hash standard deviation:  4.519106806253458              does resets when adding; not\n",
    "    double hash probe count mean:  15.733                           when searching, so that\n",
    "    double hash standard deviation:  4.2555635138981085             shouldn't be a problem\n",
    "\n",
    "    The two tests above had the same variables but different results because things aren't updating right.\n",
    "\n",
    "    (Initial Cap: 10, Enlarge when load > 0.9, Enlarge By > 1.2x)   \n",
    "    linear hash probe count mean:  16.504\n",
    "    linear hash standard deviation:  4.721469715700428\n",
    "    double hash probe count mean:  15.697\n",
    "    double hash standard deviation:  4.523898391504462\n",
    "\n",
    "    (Initial Cap: 10, Enlarge when load > 0.7, Enlarge By > 2x)\n",
    "    linear hash probe count mean:  58.495\n",
    "    linear hash standard deviation:  12.71808651978452\n",
    "    double hash probe count mean:  45.12\n",
    "    double hash standard deviation:  8.33781260997866\n",
    "\n",
    "    (Initial Cap: 10, Enlarge when load > 0.95, Enlarge By > 2x)  \n",
    "    linear hash probe count mean:  58.124\n",
    "    linear hash standard deviation:  12.719686097322308\n",
    "    double hash probe count mean:  45.002\n",
    "    double hash standard deviation:  8.20983540623059\n",
    "\n",
    "    Ok, I think something must be messed up, cause these results don't track with expected behavior. I don't think it makes sense for the number of search probes to be 3 or 4 times as high when we have a much bigger table. Either I'm having a day long brain-fart, or the enlarge and load ratio things aren't acting how I intended.\n",
    "\n",
    "    It doesn't help that the jupiter notebook doesn't seem to update correctly unless I run all the classes individually before running GetStatistics. Even \"Run All\" doesn't seem to be doing it.\n",
    "\n",
    "    The issue that has been plaguing me all day was (probably) a one character typo. I think it's fixed now\n",
    "\n",
    "    (Initial Cap: 10, Enlarge when load > 0.7, Enlarge By > 2x)  \n",
    "    linear hash probe count mean:  253.053\n",
    "    linear hash standard deviation:  44.221711305090295\n",
    "    double hash probe count mean:  161.879\n",
    "    double hash standard deviation:  22.84894819096567\n",
    "\n",
    "    Now double hash is close to 1.5, but linear hash is waaay off. I'm gonna start tweaking values again and see if they behave closer to expectations\n",
    "\n",
    "    (Initial Cap: 10, Enlarge when load > 0.9, Enlarge By > 2x) \n",
    "    linear hash probe count mean:  275.845\n",
    "    linear hash standard deviation:  122.125614187363\n",
    "    double hash probe count mean:  166.064\n",
    "    double hash standard deviation:  37.50163031491133\n",
    "\n",
    "    (Initial Cap: 10, Enlarge when load > 0.6, Enlarge By > 2x)\n",
    "    linear hash probe count mean:  60.135\n",
    "    linear hash standard deviation:  23.414076834118273\n",
    "    double hash probe count mean:  46.941\n",
    "    double hash standard deviation:  14.173652013909873\n",
    "    \n",
    "    (Initial Cap: 10, Enlarge when load > 0.5, Enlarge By > 2x)\n",
    "    linear hash probe count mean:  57.231\n",
    "    linear hash standard deviation:  12.060061529033923\n",
    "    double hash probe count mean:  45.565\n",
    "    double hash standard deviation:  8.46116814431473\n",
    "\n",
    "    There seems to be a big jump between load threshold 0.7 and 0.6 while enlarge_by > 2\n",
    "\n",
    "    (Initial Cap: 10, Enlarge when load > 0.6, Enlarge By > 1.5x)\n",
    "    linear hash probe count mean:  153.21\n",
    "    linear hash standard deviation:  25.274744562758944\n",
    "    double hash probe count mean:  104.477\n",
    "    double hash standard deviation:  14.820226385251681\n",
    "\n",
    "Woo, look at that. An enlarge threshold of 0.6 and enlarge factor of 1.5x+ gives Linear probing an average close to 1.5 probes per failed search, while it gives double hashing an average close to 1 probe per failed search. This makes sense since double hashing should result in less collisions than linear probing, but the problem summary says that we should expect them to be the same, which has me confused. I thought the whole point of double hashing was to avoid clumping in order to avoid collisions.\n",
    "\n",
    "######\n",
    "\n",
    "##### this is about where we go from troubleshooting to actually being able to run the relevant experiments\n",
    "\n",
    "###### EXPERIMENTS:\n",
    "\n",
    "\n",
    "Ok, I was trying  add some functions to collect some more data and realized the tables weren't resizing after each test so I had to fix that as well. Now results are pretty different, but we can see stuff like the end table size.\n",
    "\n",
    "    (Initial Cap: 101, Enlarge when load > 0.7, Enlarge By > 1.5x)\n",
    "    linear hash table size:          157\n",
    "    double hash table size:          157\n",
    "    linear hash probe count mean:    312.441\n",
    "    linear hash avg per search:      3.1244099999999997\n",
    "    linear hash standard deviation:  117.42401337851605\n",
    "    double hash probe count mean:    172.955\n",
    "    double hash avg per search:      1.7295500000000001\n",
    "    double hash standard deviation:  20.977918105666443\n",
    "\n",
    "    (Initial Cap: 101, Enlarge when load > 0.9, Enlarge By > 1.5x)\n",
    "    linear hash table size:          157\n",
    "    double hash table size:          157\n",
    "    linear hash probe count mean:    311.38\n",
    "    linear hash avg per search:      3.1138\n",
    "    linear hash standard deviation:  115.14922033145923\n",
    "    double hash probe count mean:    173.387\n",
    "    double hash avg per search:      1.73387\n",
    "    double hash standard deviation:  20.795665189906543\n",
    "\n",
    "    (Initial Cap: 101, Enlarge when load > 0.7, Enlarge By > 2x)\n",
    "    linear hash table size:          211\n",
    "    double hash table size:          211\n",
    "    linear hash probe count mean:    129.004\n",
    "    linear hash avg per search:      1.2900399999999999\n",
    "    linear hash standard deviation:  33.40922268117234\n",
    "    double hash probe count mean:    89.584\n",
    "    double hash avg per search:      0.8958400000000001\n",
    "    double hash standard deviation:  13.175376068049614\n",
    "\n",
    "    \n",
    "    \n",
    "In these last three tests we can see that while changing the threshold doesn't seem to have much effect, changing the size the table blows up by does, which makes sense because spreading things out more should lead to much less probing. I'm gonna try to fine tune for exact 1.5 values on each average, though it's seeming like double hashing is going to be able to do more with less. To get linear to 1.5 probes per search, I'm going to fine tune the enlarge_by number and I probably won't get exactly 1.5 since there are only so many primes that can be table sizes.\n",
    "\n",
    "    (Initial Cap: 101, Enlarge when load > 0.7, Enlarge By > 1.9x)\n",
    "    linear hash table size:          193\n",
    "    double hash table size:          193\n",
    "    linear hash probe count mean:    161.611\n",
    "    linear hash avg per search:      1.61611\n",
    "    linear hash standard deviation:  43.19174928393392\n",
    "    double hash probe count mean:    106.491\n",
    "    double hash avg per search:      1.06491\n",
    "    double hash standard deviation:  14.927114179831872\n",
    "    \n",
    "Hmm, 1.6 probes per search is an overshoot.\n",
    "\n",
    "    (Initial Cap: 101, Enlarge when load > 0.7, Enlarge By > 1.95x)\n",
    "    linear hash table size:          199\n",
    "    double hash table size:          199\n",
    "    linear hash probe count mean:    145.56\n",
    "    linear hash avg per search:      1.4556\n",
    "    linear hash standard deviation:  36.977557244087116\n",
    "    double hash probe count mean:    99.688\n",
    "    double hash avg per search:      0.99688\n",
    "    double hash standard deviation:  13.706513945001642\n",
    "\n",
    "1.45 is pretty close\n",
    "\n",
    "    (Initial Cap: 101, Enlarge when load > 0.7, Enlarge By > 1.93x)  \n",
    "    linear hash table size:          197\n",
    "    double hash table size:          197\n",
    "    linear hash probe count mean:    150.181\n",
    "    linear hash avg per search:      1.50181\n",
    "    linear hash standard deviation:  36.54035353463943\n",
    "    double hash probe count mean:    102.607\n",
    "    double hash avg per search:      1.02607\n",
    "    double hash standard deviation:  14.193801992948014\n",
    "\n",
    "\n",
    "There it is. It looks like a hash table size of 197 gives us an average of 1.5 probes per failed search with the Linear Probe method. Nearly twice as many buckets as there are entries. Now I'll tune for double to get to 1.5\n",
    "\n",
    "    (Initial Cap: 101, Enlarge when load > 0.7, Enlarge By > 1.6x) \n",
    "    linear hash table size:          163\n",
    "    double hash table size:          163\n",
    "    linear hash probe count mean:    268.974\n",
    "    linear hash avg per search:      2.68974\n",
    "    linear hash standard deviation:  85.41424305651205\n",
    "    double hash probe count mean:    156.884\n",
    "    double hash avg per search:      1.5688399999999998\n",
    "    double hash standard deviation:  19.218113327792608\n",
    "\n",
    "1.56 pps is already pretty close\n",
    "\n",
    "    (Initial Cap: 101, Enlarge when load > 0.7, Enlarge By > 1.54x)\n",
    "    linear hash table size:          157\n",
    "    double hash table size:          157\n",
    "    linear hash probe count mean:    315.024\n",
    "    linear hash avg per search:      3.15024\n",
    "    linear hash standard deviation:  111.91790313011158\n",
    "    double hash probe count mean:    171.807\n",
    "    double hash avg per search:      1.71807\n",
    "    double hash standard deviation:  21.689356356959493\n",
    "\n",
    "157 is the next prime, but it overshoots the desired 1.5 PPS. 1.56 PPS is as close as it's gonna get. With a table size of 163, we get nearly 1.5 pps with the Double Hash method. Next I'll bump up to a 1000 name search\n",
    "\n",
    "    (Initial Cap: 101, Enlarge when load > 0.7, Enlarge By > 1.5x) Searching for 1000 names instead of 100\n",
    "    linear hash table size:          157\n",
    "    double hash table size:          157\n",
    "    linear hash probe count mean:    3084.002\n",
    "    linear hash avg per search:      3.084002\n",
    "    linear hash standard deviation:  1052.9406754713496\n",
    "    double hash probe count mean:    1716.576\n",
    "    double hash avg per search:      1.716576\n",
    "    double hash standard deviation:  68.11275630578733\n",
    "\n",
    "Program took much longer to run when the search list was increased tenfold, but that makes sense I suppose. Runtime quadrupled to over 3 seconds rather than decupling, which suggests to me that the search is not the most time consuming process going on. I also feel like this runtime might be quite high, which makes me nervous because I've been at this for a hot minute and I haven't seen any big speed ups or errors that could be causing things to run slow. Probably the stupid name generator. The biggest difference is the standard deviation, which is like 10x for linear\n",
    "\n",
    "    (Initial Cap: 101, Enlarge when load > 0.3, Enlarge By > 1.5x) Searching for 1000 names instead of 100\n",
    "    linear hash table size:          359\n",
    "    double hash table size:          359\n",
    "    linear hash probe count mean:    459.243\n",
    "    linear hash avg per search:      0.459243\n",
    "    linear hash standard deviation:  52.311092527315864\n",
    "    double hash probe count mean:    385.154\n",
    "    double hash avg per search:      0.385154\n",
    "    double hash standard deviation:  23.031385731838384\n",
    "\n",
    "    (Initial Cap: 101, Enlarge when load > 0.1, Enlarge By > 1.5x) Searching for 1000 names instead of 100\n",
    "    linear hash table size:          1237\n",
    "    double hash table size:          1237\n",
    "    linear hash probe count mean:    93.037\n",
    "    linear hash avg per search:      0.09303700000000001\n",
    "    linear hash standard deviation:  11.367887220581487\n",
    "    double hash probe count mean:    88.213\n",
    "    double hash avg per search:      0.088213\n",
    "    double hash standard deviation:  9.911299857479081\n",
    "\n",
    "The above is the only way I've found to make linear and double hashing yield similar probe counts, but it's just by making collisions super unlikely by keeping a way oversized table more than 10 times the \n",
    "\n",
    "    (Initial Cap: 101, Enlarge when load > 0.95, Enlarge By > 1.05x) Searching for 1000 names instead of 100\n",
    "    linear hash table size:          107\n",
    "    double hash table size:          107\n",
    "    linear hash probe count mean:    27660.758\n",
    "    linear hash avg per search:      27.660758\n",
    "    linear hash standard deviation:  9022.12239941501\n",
    "    double hash probe count mean:    12531.228\n",
    "    double hash avg per search:      12.531227999999999\n",
    "    double hash standard deviation:  412.00776985199974 \n",
    "\n",
    "Restricting the list size to nearly the number of items doesn't help anything. This took 13.0 seconds to run\n",
    "\n",
    "\n",
    "##    FINAL NOTES: \n",
    "I'm not sure if I did something wrong. The problem text says we should expect linear and double hash collision resolution to yield the same average number of probes per failed search, but that doesn't make sense to me. As stated in notes above, I thought the whole point of double hashing was to prevent the clumping that linear hashing is subject to. Since there's an algorithm to take a pseudo-random number of steps after a collision, elements in the hash table should be more spread out and we don't have to probe through a clump before finding an empty bucket. With that in mind, I think it makes sense that the double hash method didn't need nearly as many probes per failed search (hereafter reffered to as PPS). Constrainging the table size and increasing the enlarge threshold didn't make the resulting PPS averages any closer either. The data I've collected suggests that the double hashing method is able to achieve nearly 1.5 PPS with a table size of only 163, while the linear hash method needs a table of size 197 to get to 1.5 PPS. I'm unsure if I've goofed something up because of the problem text and example output that suggests they should be the same, but all of my tests have shown double hashing to use less probes, and that seems correct to me. I thought that's why we double hash.\n",
    "\n",
    "When I increased the search list from 100 to 1000, the averages came out similar to before but interestingly standard deviation goes up quite a bit. It goes up for both, but much more so for Linear Hash which needs some explanation. I think this might be because of the exponents in the standard deviation math, which might cause a larger jump for the larger average difference, but unfortunately I haven't taken statistics yet so I could be way off there. Additionally, I think Linear Probing might have a larger deviation range because of clumping. Whether a search takes a lot of probing or not depends entirely on whether it hits a clump with its initial probe, and the larger a clump gets the more likely it is to be hit, and the more probing it's going to take to escape the clump. All that might lead to a larger range of outliers, or more distribution away from the mean. Again though, that could all be malarky because I have very little experience with statistics.\n",
    "\n",
    "The only way I was able to make the mean probe counts similar for linear and double hash was by making the threshold for table expansion extremely low. This ensured a table with many more buckets than items which I suspect worked because it made collisions much less likely. If linear hash never makes an initial collision clumps won't form and I think that might make it perform much closer to double hash in mean probe count. Still not quite as good though, apparently.\n",
    "\n",
    "In conclusion, Double Hashing seems like the way to go per my testing and tinkering. On average it gave much lower probe counts when searching for items within the table. I might've bungled something, but if I have I can't figure out what and I've been at it all day. That and the above reasoning for double hashing being faster makes me feel like I probably haven't goofed it up, but if I have I really must know where I went wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "END EXPERIMENT NOTES;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " below is the blooper reel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n        found_key = False\\n        while not found_key:           # until we find the key\\n            if(self.table[index][0]==key):  # check if our key matches the key in that index\\n                found_key = True            # if it does, end loop\\n            elif(self.table[index] == None):# if we hit an empty\\n                return -1                   # -1, item not found\\n            else:                           # else\\n                # modulo get_size lets us wrap back around if we hit the end of the array\\n                index = (index+1) % self.get_size# increment index\\n        return index                        # return the index if found\\n'"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"     This one has a silly flag and some other goofy mistakes\n",
    "        found_key = False\n",
    "        while not found_key:           # until we find the key\n",
    "            if(self.table[index][0]==key):  # check if our key matches the key in that index\n",
    "                found_key = True            # if it does, end loop\n",
    "            elif(self.table[index] == None):# if we hit an empty\n",
    "                return -1                   # -1, item not found\n",
    "            else:                           # else\n",
    "                # modulo get_size lets us wrap back around if we hit the end of the array\n",
    "                index = (index+1) % self.get_size# increment index\n",
    "        return index                        # return the index if found\n",
    "\"\"\"\n",
    "# When this project started, I tried for a whole different name generator that would make\n",
    "# names that actually made sense rather than random strings, but it ended up being far too\n",
    "# convoluted and I had to trash it. The simplified one I ended up using instead is still\n",
    "# way more complicated than it needs to be becaues it brute forces through every\n",
    "# possible char string of a certain length rather than just returning a list full of\n",
    "# \"Name_{number}\" or whathaveyou\n",
    "\n",
    "# print(\"EEEEEEEEEEEEEEEeeeeeeeeeeeeeeeEEEEEEEEEEEEEEE\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
